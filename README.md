# fomo
tmp repo for holding fomo stuff

# Pre-processing/Feature Engineering


# Gradient boosting & Random forest
[TL;DR](http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/) Gradient boosting is "better", but random forest is easier to tune, and maybe faster. Additionally, gradient boosting have trouble when the training data is noisy.

* [Nice overview of ensemble methods](https://medium.com/@aravanshad/ensemble-methods-95533944783f)
* [A more in-depth review of the strengths and weaknesses of GBM vs RF](https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80)

## Gradient Boosting
* [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html) has nice documentation, [plenty of examples and tutorials](https://github.com/dmlc/xgboost/tree/master/demo) and R & python interfaces. It seems this is a very strong ML package, but could be more difficult to use than other options.
* [Another explanation of GBM (including a nice visual representation))[https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/]. Also goes into extensive depth explaining all the parameters.

# Deep learning
* Zhou et al 2018 [A primer on deep learning in genomics](https://sci-hub.tw/https://www.nature.com/articles/s41588-018-0295-5)
